services:
  # ========== HADOOP ECOSYSTEM ==========
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      CLUSTER_NAME: "goszakupki"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
      HDFS_CONF_dfs_replication: 2
    volumes:
      - ./data/hdfs/namenode:/hadoop/dfs/name
    networks:
      - goszakupki_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
    volumes:
      - ./data/hdfs/datanode1:/hadoop/dfs/data
    networks:
      - goszakupki_network
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: "hdfs://namenode:9000"
    volumes:
      - ./data/hdfs/datanode2:/hadoop/dfs/data
    networks:
      - goszakupki_network
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9864"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== SPARK MASTER & WORKERS ==========
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    restart: always
    ports:
      - "8080:8080"
      - "7077:7077"
      - "6066:6066"
    environment:
      MASTER: "spark-master"
      SPARK_CONF_SPARK_DRIVER_MEMORY: "2g"
      SPARK_CONF_SPARK_EXECUTOR_MEMORY: "2g"
    networks:
      - goszakupki_network
    depends_on:
      - namenode
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-worker1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker1
    restart: always
    ports:
      - "8081:8081"
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: "2G"
    networks:
      - goszakupki_network
    depends_on:
      - spark-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-worker2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker2
    restart: always
    ports:
      - "8082:8082"
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: "2G"
    networks:
      - goszakupki_network
    depends_on:
      - spark-master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== NIFI (для ETL потоков) ==========
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    restart: always
    ports:
      - "8090:8080"
    environment:
      NIFI_WEB_HTTP_HOST: "0.0.0.0"
      NIFI_WEB_HTTP_PORT: "8080"
      NIFI_CLUSTER_IS_NODE: "true"
      NIFI_CLUSTER_NODE_ADDRESS: "nifi"
      NIFI_ZK_CONNECT_STRING: "zookeeper:2181"
    volumes:
      - ./data/nifi:/opt/nifi/nifi-current/conf
    networks:
      - goszakupki_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/nifi"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== ZOOKEEPER (для координации) ==========
  zookeeper:
    image: zookeeper:3.9
    container_name: zookeeper
    restart: always
    environment:
      ZOO_CFG_EXTRA: "tickTime=2000"
    ports:
      - "2181:2181"
    networks:
      - goszakupki_network
    healthcheck:
      test: ["CMD", "echo", "ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ========== POSTGRES для приложения ==========
  postgres:
    image: postgres:13
    container_name: postgres
    restart: always
    environment:
      POSTGRES_DB: "goszakupki"
      POSTGRES_USER: "zakupki"
      POSTGRES_PASSWORD: "zakupki_password"
    ports:
      - "5432:5432"
    volumes:
      - ./data/postgresql/app:/var/lib/postgresql/data
    networks:
      - goszakupki_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U zakupki"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ========== REDIS для кеша ==========
  redis:
    image: redis:7-alpine
    container_name: redis
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis:/data
    networks:
      - goszakupki_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ========== JUPYTER NOTEBOOK для анализа ==========
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      SPARK_MASTER: "spark://spark-master:7077"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/home/jovyan/data
    networks:
      - goszakupki_network
    depends_on:
      - spark-master
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  goszakupki_network:
    driver: bridge

volumes:
  hdfs_namenode:
  hdfs_datanode1:
  hdfs_datanode2:
  postgres:
  redis:
