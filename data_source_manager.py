"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö - –ø–∞—Ä—Å–µ—Ä RSS –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö
"""
import feedparser
import requests
import pandas as pd
import logging
import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import sqlite3
import hashlib
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RSSFeedParser:
    """–ü–∞—Ä—Å–µ—Ä RSS –ª–µ–Ω—Ç—ã –∑–∞–∫—É–ø–æ–∫.gov.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π VPN –∏ retry"""
    
    def __init__(self):
        self.feed_url = "https://zakupki.gov.ru/epz/order/extendedsearch/rss.html"
        self.timeout = 60  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è VPN
        self.session = self._create_session()
        self.cache_file = "./data/.rss_cache.json"
        self.cache_ttl = 600  # 10 –º–∏–Ω—É—Ç –∫—ç—à
        
    def _create_session(self):
        """–°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é —Å retry –ª–æ–≥–∏–∫–æ–π"""
        session = requests.Session()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è retry
        retry_strategy = Retry(
            total=5,  # 5 –ø–æ–ø—ã—Ç–æ–∫
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],
            backoff_factor=2  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff: 2, 4, 8, 16, 32 —Å–µ–∫
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        return session
    
    def _get_cached_feed(self) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RSS"""
        try:
            if not os.path.exists(self.cache_file):
                return None
            
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
            cached_time = datetime.fromisoformat(cache.get('time', ''))
            if (datetime.now() - cached_time).total_seconds() < self.cache_ttl:
                logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RSS (–≤–æ–∑—Ä–∞—Å—Ç: {(datetime.now() - cached_time).total_seconds():.0f}—Å–µ–∫)")
                return cache.get('feed')
        except:
            pass
        
        return None
    
    def _save_cache(self, feed_text: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å RSS –≤ –∫—ç—à"""
        try:
            os.makedirs("./data", exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'time': datetime.now().isoformat(),
                    'feed': feed_text
                }, f, ensure_ascii=False)
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")
    
    def fetch_feed(self) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å RSS –ª–µ–Ω—Ç—É —Å retry –ø—Ä–∏ VPN –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            logger.info(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ RSS –ª–µ–Ω—Ç—ã: {self.feed_url}")
            logger.info(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç: {self.timeout}—Å–µ–∫, –ü–æ–ø—ã—Ç–∫–∏: 5")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'application/rss+xml,application/atom+xml,text/html;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # –û—Ç–∫–ª—é—á–∞–µ–º SSL –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è VPN (–º–æ–∂–µ—Ç –±—ã—Ç—å MITM)
            response = self.session.get(
                self.feed_url, 
                headers=headers, 
                timeout=self.timeout,
                verify=False  # –î–ª—è VPN
            )
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                feed = feedparser.parse(response.text)
                logger.info(f"‚úÖ RSS –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ. –ó–∞–ø–∏—Å–µ–π: {len(feed.entries)}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self._save_cache(response.text)
                
                return feed
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS: —Å—Ç–∞—Ç—É—Å {response.status_code}")
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞
                cached_feed = self._get_cached_feed()
                if cached_feed:
                    logger.info("üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
                    return feedparser.parse(cached_feed)
                return None
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ RSS: {type(e).__name__}: {e}")
            logger.info("üí° –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VPN, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–∫–ª—é—á–∏—Ç—å –µ–≥–æ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å")
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            cached_feed = self._get_cached_feed()
            if cached_feed:
                logger.info("üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RSS –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
                return feedparser.parse(cached_feed)
            
            return None
    
    def parse_entry(self, entry) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç—å –æ–¥–Ω—É –∑–∞–ø–∏—Å—å RSS"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS
            title = entry.get('title', '')
            link = entry.get('link', '')
            published = entry.get('published', '')
            description = entry.get('description', '')
            
            # –ü–∞—Ä—Å–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π
            details = self._parse_description(description)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä –∑–∞–∫—É–ø–∫–∏ –∏–∑ —Å—Å—ã–ª–∫–∏
            nomer = self._extract_purchase_number(link)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            purchase = {
                'nomer': nomer,
                'organizaciya': details.get('organizaciya', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                'opisanie': title[:200],  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                'kategoriya': details.get('kategoriya', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                'region': details.get('region', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                'byudzhet': details.get('byudzhet', 0),
                'status': '–û–±—ä—è–≤–ª–µ–Ω–∞',
                'data': self._parse_date(published),
                'istochnik': 'zakupki.gov.ru',
                'url': link,
                'raw_description': description,
                'loaded_at': datetime.now().isoformat()
            }
            
            return purchase
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏: {e}")
            return None
    
    def _parse_description(self, description: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç—å HTML –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π"""
        details = {
            'organizaciya': '–ù–µ —É–∫–∞–∑–∞–Ω–∞',
            'kategoriya': '–ù–µ —É–∫–∞–∑–∞–Ω–∞',
            'region': '–ù–µ —É–∫–∞–∑–∞–Ω–∞',
            'byudzhet': 0
        }
        
        try:
            soup = BeautifulSoup(description, 'html.parser')
            text = soup.get_text()
            
            # –ü–∞—Ä—Å–∏–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            if '–ó–∞–∫–∞–∑—á–∏–∫:' in text:
                org_match = re.search(r'–ó–∞–∫–∞–∑—á–∏–∫:\s*([^\n<]+)', text)
                if org_match:
                    details['organizaciya'] = org_match.group(1).strip()[:100]
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–≥–∏–æ–Ω
            if '–†–µ–≥–∏–æ–Ω:' in text:
                reg_match = re.search(r'–†–µ–≥–∏–æ–Ω:\s*([^\n<]+)', text)
                if reg_match:
                    details['region'] = reg_match.group(1).strip()[:50]
            
            # –ü–∞—Ä—Å–∏–º –ù–ú–¶–∫
            if '–ù–∞—á–∞–ª—å–Ω–∞—è' in text or '–ù–∞—á–∞–ª—å–Ω–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞' in text:
                price_match = re.search(r'–ù–∞—á–∞–ª—å–Ω–∞—è[^:]*:\s*([0-9,.\s]+)', text)
                if price_match:
                    price_str = price_match.group(1).replace(' ', '').replace(',', '.')
                    try:
                        details['byudzhet'] = float(price_str)
                    except:
                        pass
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
            categories = {
                '–ú–µ–¥–∏—Ü–∏–Ω–∞': ['–º–µ–¥–∏—Ü–∏–Ω', '—Ñ–∞—Ä–º–∞—Ü', '–∑–¥–æ—Ä–æ–≤—å', '–±–æ–ª—å–Ω–∏—Ü', '–∫–ª–∏–Ω–∏–∫'],
                '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç': ['–∞–≤—Ç–æ', '–º–∞—à–∏–Ω', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–¥–æ—Ä–æ–≥'],
                '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ': ['—Å—Ç—Ä–æ–∏', '—Ä–µ–º–æ–Ω—Ç', '—Å–º–µ—Ç–Ω', '–ø—Ä–æ–µ–∫—Ç–Ω'],
                '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞': ['—ç–ª–µ–∫—Ç—Ä', '—ç–Ω–µ—Ä–≥', '–≥–∞–∑'],
                'IT': ['–ø—Ä–æ–≥—Ä–∞–º–º', '—Å–æ—Ñ—Ç', '–∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–µ—Ä–≤–µ—Ä', '—Å–∏—Å—Ç–µ–º'],
                '–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': ['–æ–±—Ä–∞–∑–æ–≤', '—É—á–µ–±', '—à–∫–æ–ª', '—É–Ω–∏–≤–µ—Ä—Å–∏'],
                '–ñ–∏–ª—å–µ': ['–∂–∏–ª', '–¥–æ–º', '–∫–≤–∞—Ä—Ç–∏—Ä'],
                '–ë–ª–∞–≥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ': ['–ø–∞—Ä–∫', '–æ–∑–µ–ª–µ–Ω', '—Å–∫–≤–µ—Ä—ã', '–¥–æ—Ä–æ–≥'],
            }
            
            text_lower = text.lower()
            for cat, keywords in categories.items():
                if any(kw in text_lower for kw in keywords):
                    details['kategoriya'] = cat
                    break
                    
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
        
        return details
    
    def _extract_purchase_number(self, link: str) -> str:
        """–ò–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –∑–∞–∫—É–ø–∫–∏ –∏–∑ URL"""
        try:
            match = re.search(r'registrationNumber=([^&]+)', link)
            if match:
                return f"44-{match.group(1)}"
        except:
            pass
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        return f"44-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    
    def _parse_date(self, date_str: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∏–∑ RSS"""
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                try:
                    dt = datetime.strptime(date_str.split('+')[0].split('Z')[0].strip(), fmt)
                    return dt.strftime('%Y-%m-%d')
                except:
                    continue
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –µ—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ —Å–ø–∞—Ä—Å–∏—Ç—å
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã '{date_str}': {e}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def get_new_entries(self, last_load_time: Optional[datetime] = None) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
        feed = self.fetch_feed()
        if not feed or not feed.entries:
            return []
        
        entries = []
        for entry in feed.entries[:100]:  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100
            parsed = self.parse_entry(entry)
            if parsed:
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±–µ—Ä–µ–º –≤—Å–µ
                if last_load_time is None:
                    entries.append(parsed)
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É
                    try:
                        entry_date = datetime.fromisoformat(parsed['data'])
                        if entry_date > last_load_time:
                            entries.append(parsed)
                    except:
                        entries.append(parsed)
        
        return entries


class DataSourceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, db_path="./data/zakupki.db"):
        self.db_path = db_path
        self.rss_parser = RSSFeedParser()
        self._init_db()
    
    def _init_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –¢–∞–±–ª–∏—Ü–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS data_sources (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE,
                    type TEXT,
                    url TEXT,
                    last_load_time TEXT,
                    last_load_count INTEGER,
                    status TEXT,
                    created_at TEXT
                )
            """)
            
            # –¢–∞–±–ª–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS loaded_purchases (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    nomer TEXT UNIQUE,
                    source TEXT,
                    loaded_at TEXT,
                    data_hash TEXT
                )
            """)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É etl_raw –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è source_id
            cursor.execute("""
                ALTER TABLE etl_raw ADD COLUMN source_id INTEGER DEFAULT NULL
            """)
            
            conn.commit()
            conn.close()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ RSS –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            self._init_rss_source()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
    
    def _init_rss_source(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RSS –∏—Å—Ç–æ—á–Ω–∏–∫"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR IGNORE INTO data_sources 
                (name, type, url, status, created_at)
                VALUES (?, ?, ?, ?, ?)
            """, (
                'RSS - –∑–∞–∫—É–ø–∫–∏.gov.ru',
                'rss',
                self.rss_parser.feed_url,
                'active',
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {e}")
    
    def _get_data_hash(self, data: Dict) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ö–µ—à –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        content = f"{data['nomer']}{data['opisanie']}{data['byudzhet']}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_duplicate(self, nomer: str, data_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ —É–∂–µ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute(
                "SELECT id FROM loaded_purchases WHERE nomer = ? AND data_hash = ?",
                (nomer, data_hash)
            )
            
            result = cursor.fetchone()
            conn.close()
            
            return result is not None
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return False
    
    def load_rss_data(self, limit: int = 50) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS –ª–µ–Ω—Ç—ã"""
        logger.info("üîÑ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ RSS...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
            entries = self.rss_parser.get_new_entries()[:limit]
            
            if not entries:
                logger.info("üì≠ –ù–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return {
                    'status': 'no_data',
                    'loaded_count': 0,
                    'duplicate_count': 0,
                    'error_count': 0
                }
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –ë–î
            from etl_manager import ETLDataManager
            etl_manager = ETLDataManager(self.db_path)
            
            loaded_count = 0
            duplicate_count = 0
            error_count = 0
            
            for entry in entries:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                    data_hash = self._get_data_hash(entry)
                    if self._is_duplicate(entry['nomer'], data_hash):
                        duplicate_count += 1
                        continue
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
                    df = pd.DataFrame([entry])
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ raw —Å–ª–æ–π
                    etl_manager.load_raw_data(df, source='RSS - –∑–∞–∫—É–ø–∫–∏.gov.ru')
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö
                    conn = sqlite3.connect(self.db_path)
                    cursor = conn.cursor()
                    cursor.execute("""
                        INSERT INTO loaded_purchases (nomer, source, loaded_at, data_hash)
                        VALUES (?, ?, ?, ?)
                    """, (entry['nomer'], 'rss', datetime.now().isoformat(), data_hash))
                    conn.commit()
                    conn.close()
                    
                    loaded_count += 1
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–ø–∏—Å–∏: {e}")
                    error_count += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            self._update_source_info('RSS - –∑–∞–∫—É–ø–∫–∏.gov.ru', loaded_count)
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {loaded_count} –Ω–æ–≤—ã—Ö, {duplicate_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, {error_count} –æ—à–∏–±–æ–∫")
            
            return {
                'status': 'success',
                'loaded_count': loaded_count,
                'duplicate_count': duplicate_count,
                'error_count': error_count
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ RSS: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'loaded_count': 0,
                'duplicate_count': 0,
                'error_count': 0
            }
    
    def _update_source_info(self, source_name: str, count: int):
        """–û–±–Ω–æ–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE data_sources 
                SET last_load_time = ?, last_load_count = ?, status = ?
                WHERE name = ?
            """, (datetime.now().isoformat(), count, 'active', source_name))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ: {e}")
    
    def get_source_stats(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT name, type, last_load_time, last_load_count, status
                FROM data_sources
                ORDER BY last_load_time DESC
            """)
            
            results = cursor.fetchall()
            conn.close()
            
            stats = []
            for row in results:
                stats.append({
                    'name': row[0],
                    'type': row[1],
                    'last_load_time': row[2],
                    'last_load_count': row[3],
                    'status': row[4]
                })
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return []
    
    def get_duplicates_count(self, hours: int = 24) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            since = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            cursor.execute("""
                SELECT COUNT(*) FROM loaded_purchases 
                WHERE loaded_at > ?
            """, (since,))
            
            count = cursor.fetchone()[0]
            conn.close()
            
            return count
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            return 0


if __name__ == '__main__':
    # –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞
    manager = DataSourceManager()
    result = manager.load_rss_data(limit=10)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = manager.get_source_stats()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats}")
