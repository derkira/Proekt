"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π ETL –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–º–∏–ª–ª–∏–æ–Ω—ã –∑–∞–ø–∏—Å–µ–π)
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ HDFS, Hive, Spark –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime, timedelta
import json

import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, count, sum as spark_sum, avg, max as spark_max, 
    min as spark_min, desc, asc, year, month, dayofmonth,
    row_number, broadcast, coalesce, concat_ws
)
from pyspark.sql.window import Window
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.regression import LinearRegression
import pyarrow as pa
import pyarrow.parquet as pq

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BigDataHDFSManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å HDFS –∏ –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.fs = self.spark.sparkContext._jsc.hadoopConfiguration()
        
    def read_hdfs_parquet(self, path: str) -> 'pyspark.sql.DataFrame':
        """–ß—Ç–µ–Ω–∏–µ Parquet —Ñ–∞–π–ª–æ–≤ –∏–∑ HDFS"""
        try:
            df = self.spark.read.parquet(path)
            logger.info(f"‚úì Loaded from HDFS: {path} ({df.count()} rows)")
            return df
        except Exception as e:
            logger.error(f"‚úó Failed to read HDFS: {e}")
            return None
    
    def write_hdfs_parquet(self, df: 'pyspark.sql.DataFrame', path: str, mode: str = 'overwrite'):
        """–ó–∞–ø–∏—Å—å Parquet —Ñ–∞–π–ª–æ–≤ –≤ HDFS"""
        try:
            df.write.mode(mode).parquet(path)
            row_count = df.count()
            logger.info(f"‚úì Written to HDFS: {path} ({row_count} rows)")
            return True
        except Exception as e:
            logger.error(f"‚úó Failed to write HDFS: {e}")
            return False
    
    def read_hdfs_csv(self, path: str, header: bool = True) -> 'pyspark.sql.DataFrame':
        """–ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–æ–≤ –∏–∑ HDFS"""
        try:
            df = self.spark.read.csv(path, header=header, inferSchema=True)
            logger.info(f"‚úì Loaded CSV from HDFS: {path}")
            return df
        except Exception as e:
            logger.error(f"‚úó Failed to read CSV from HDFS: {e}")
            return None
    
    def write_hive_table(self, df: 'pyspark.sql.DataFrame', table_name: str, mode: str = 'overwrite'):
        """–ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ Hive —Ç–∞–±–ª–∏—Ü—É"""
        try:
            df.write.mode(mode).format('hive').saveAsTable(table_name)
            logger.info(f"‚úì Written to Hive table: {table_name}")
            return True
        except Exception as e:
            logger.error(f"‚úó Failed to write to Hive: {e}")
            return False


class DataCleaner:
    """–û—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"""
    
    @staticmethod
    def remove_duplicates(df: 'pyspark.sql.DataFrame', subset: List[str]) -> 'pyspark.sql.DataFrame':
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å—Ç–æ–ª–±—Ü–∞–º"""
        initial_count = df.count()
        df_clean = df.dropDuplicates(subset)
        final_count = df_clean.count()
        logger.info(f"üìä Duplicates removed: {initial_count - final_count} rows")
        return df_clean
    
    @staticmethod
    def handle_nulls(df: 'pyspark.sql.DataFrame', strategy: str = 'drop') -> 'pyspark.sql.DataFrame':
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ NULL –∑–Ω–∞—á–µ–Ω–∏–π"""
        if strategy == 'drop':
            df_clean = df.na.drop()
        elif strategy == 'mean':
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
            numeric_cols = [c for c, t in df.dtypes if 'int' in t or 'double' in t]
            means = df.select([avg(c) for c in numeric_cols]).collect()[0]
            fill_dict = {numeric_cols[i]: means[i] for i in range(len(numeric_cols))}
            df_clean = df.fillna(fill_dict)
        else:
            df_clean = df
        
        logger.info(f"üìä Nulls handled with strategy: {strategy}")
        return df_clean
    
    @staticmethod
    def filter_invalid_records(df: 'pyspark.sql.DataFrame', rules: Dict) -> 'pyspark.sql.DataFrame':
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º"""
        for col_name, condition in rules.items():
            df = df.filter(condition)
        
        logger.info(f"üìä Invalid records filtered")
        return df


class BigDataETLPipeline:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π ETL –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    def __init__(self, app_name: str = "GosZakupki-BigData-ETL"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.adaptive.skewJoin.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.default.parallelism", "4") \
            .config("spark.sql.shuffle.partitions", "200") \
            .config("spark.memory.fraction", "0.8") \
            .config("spark.memory.storageFraction", "0.5") \
            .config("spark.sql.parquet.compression.codec", "snappy") \
            .enableHiveSupport() \
            .getOrCreate()
        
        self.hdfs_manager = BigDataHDFSManager(self.spark)
        self.data_cleaner = DataCleaner()
        self.data_lake_path = os.getenv('DATA_LAKE_PATH', './data/lake')
        self.hdfs_path = os.getenv('HDFS_PATH', 'hdfs://namenode:9000/user/hive/warehouse')
        
        logger.info("‚úì BigDataETLPipeline initialized")
    
    def extract_raw_data(self, source_path: str) -> 'pyspark.sql.DataFrame':
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        logger.info(f"üì• Extracting data from: {source_path}")
        
        if source_path.endswith('.csv'):
            df = self.spark.read.csv(source_path, header=True, inferSchema=True)
        elif source_path.endswith('.parquet'):
            df = self.spark.read.parquet(source_path)
        elif source_path.endswith('.json'):
            df = self.spark.read.json(source_path)
        else:
            raise ValueError(f"Unsupported file format: {source_path}")
        
        row_count = df.count()
        logger.info(f"‚úì Extracted {row_count:,} rows")
        return df
    
    def transform_large_dataset(self, df: 'pyspark.sql.DataFrame') -> 'pyspark.sql.DataFrame':
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        logger.info("üîÑ Starting data transformation")
        
        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        df = self.data_cleaner.remove_duplicates(df, subset=['id'])
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ NULL –∑–Ω–∞—á–µ–Ω–∏–π
        df = self.data_cleaner.handle_nulls(df, strategy='drop')
        
        # 3. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        df = df.select([
            when(col(c).cast("double").isNotNull(), col(c).cast("double"))
            .otherwise(col(c))
            .alias(c) if dtype.startswith('string') else col(c)
            for c, dtype in df.dtypes
        ])
        
        # 4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ–º—ã—Ö –ø–æ–ª–µ–π
        if 'sum_value' in df.columns and 'quantity' in df.columns:
            df = df.withColumn(
                'unit_price',
                coalesce(col('sum_value') / col('quantity'), 0)
            )
        
        # 5. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if 'date' in df.columns:
            df = df \
                .withColumn('year', year(col('date'))) \
                .withColumn('month', month(col('date'))) \
                .withColumn('day', dayofmonth(col('date')))
        
        logger.info(f"‚úì Transformation completed")
        return df
    
    def aggregate_by_dimensions(self, df: 'pyspark.sql.DataFrame') -> 'pyspark.sql.DataFrame':
        """–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º"""
        logger.info("üìä Aggregating data by dimensions")
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ —Ä–µ–≥–∏–æ–Ω–∞–º
        aggregated = df.groupBy('category', 'region') \
            .agg(
                count('*').alias('count'),
                spark_sum('sum_value').alias('total_value'),
                avg('sum_value').alias('avg_value'),
                spark_max('sum_value').alias('max_value'),
                spark_min('sum_value').alias('min_value')
            ) \
            .orderBy(desc('total_value'))
        
        logger.info(f"‚úì Aggregated {aggregated.count()} groups")
        return aggregated
    
    def apply_window_functions(self, df: 'pyspark.sql.DataFrame') -> 'pyspark.sql.DataFrame':
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞–Ω–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info("üìà Applying window functions")
        
        window_spec = Window.partitionBy('category').orderBy(desc('sum_value'))
        df_with_rank = df.withColumn('rank', row_number().over(window_spec))
        
        logger.info(f"‚úì Window functions applied")
        return df_with_rank
    
    def load_to_data_lake(self, df: 'pyspark.sql.DataFrame', layer: str = 'processed'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ Data Lake"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = f"{self.data_lake_path}/{layer}/data_{timestamp}"
        
        try:
            df.write.mode('overwrite').parquet(path)
            logger.info(f"‚úì Data loaded to Data Lake: {path}")
            return path
        except Exception as e:
            logger.error(f"‚úó Failed to load to Data Lake: {e}")
            return None
    
    def load_to_hive(self, df: 'pyspark.sql.DataFrame', table_name: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Hive –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            df.write.mode('overwrite').format('hive').saveAsTable(table_name)
            logger.info(f"‚úì Data loaded to Hive table: {table_name}")
            return True
        except Exception as e:
            logger.error(f"‚úó Failed to load to Hive: {e}")
            return False
    
    def get_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        stats = {
            'timestamp': datetime.now().isoformat(),
            'spark_version': self.spark.version,
            'app_name': self.spark.sparkContext.appName,
            'default_parallelism': self.spark.sparkContext.defaultParallelism,
            'executors': {
                'count': self.spark.sparkContext.defaultMinPartitions,
                'memory': self.spark.conf.get('spark.executor.memory', 'default'),
            }
        }
        logger.info(f"üìä Statistics: {json.dumps(stats, indent=2)}")
        return stats
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark —Å–µ—Å—Å–∏–∏"""
        self.spark.stop()
        logger.info("‚úì Spark session stopped")


class ScalabilityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, pipeline: BigDataETLPipeline):
        self.pipeline = pipeline
        self.results = []
    
    def generate_test_data(self, size: int) -> 'pyspark.sql.DataFrame':
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"üîß Generating test data: {size:,} rows")
        
        spark = self.pipeline.spark
        data = [
            (
                i,
                f"Tender_{i % 1000}",
                f"Category_{i % 50}",
                f"Region_{i % 30}",
                np.random.randint(1000, 100000000),
                np.random.randint(1, 100),
                f"2024-{(i % 12) + 1:02d}-{(i % 28) + 1:02d}",
                f"Org_{i % 200}",
                np.random.choice(['Open', 'Restricted', 'Cancelled'], 1)[0]
            )
            for i in range(size)
        ]
        
        schema = """
            id INT, tender_name STRING, category STRING, region STRING,
            sum_value LONG, quantity INT, date STRING, organization STRING, status STRING
        """
        
        df = spark.createDataFrame(data, schema=schema)
        logger.info(f"‚úì Generated test data: {df.count():,} rows")
        return df
    
    def measure_performance(self, data_sizes: List[int]) -> Dict:
        """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info(f"‚è±Ô∏è  Measuring performance for sizes: {data_sizes}")
        
        results = {}
        
        for size in data_sizes:
            logger.info(f"\n{'='*50}")
            logger.info(f"Testing with {size:,} rows")
            logger.info(f"{'='*50}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            df = self.generate_test_data(size)
            
            # –ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            start_time = datetime.now()
            df_transformed = self.pipeline.transform_large_dataset(df)
            df_transformed.count()  # Force execution
            transform_time = (datetime.now() - start_time).total_seconds()
            
            # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            start_time = datetime.now()
            df_aggregated = self.pipeline.aggregate_by_dimensions(df_transformed)
            df_aggregated.count()  # Force execution
            aggregate_time = (datetime.now() - start_time).total_seconds()
            
            # –ò—Ç–æ–≥–æ–≤–æ–µ –≤—Ä–µ–º—è
            total_time = transform_time + aggregate_time
            
            results[size] = {
                'size': size,
                'transform_time': transform_time,
                'aggregate_time': aggregate_time,
                'total_time': total_time,
                'rows_per_second': size / total_time if total_time > 0 else 0,
                'time_per_million': (total_time / size) * 1_000_000 if size > 0 else 0
            }
            
            logger.info(f"‚úì Transform: {transform_time:.2f}s, Aggregate: {aggregate_time:.2f}s")
            logger.info(f"‚úì Total: {total_time:.2f}s ({results[size]['rows_per_second']:.0f} rows/sec)")
        
        self.results = results
        return results
    
    def analyze_scalability(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏"""
        if not self.results:
            logger.warning("No results to analyze")
            return {}
        
        sizes = sorted(self.results.keys())
        times = [self.results[s]['total_time'] for s in sizes]
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏
        if len(sizes) >= 2:
            # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
            x = np.array(sizes).reshape(-1, 1)
            y = np.array(times)
            
            # –ü—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞: y = a * x
            a = np.sum(x.flatten() * y) / np.sum(x.flatten() ** 2)
            
            # R-squared
            y_pred = a * x.flatten()
            ss_res = np.sum((y - y_pred) ** 2)
            ss_tot = np.sum((y - np.mean(y)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
            
            analysis = {
                'coefficient': a,
                'r_squared': r_squared,
                'is_linear': r_squared > 0.95,
                'linearity_score': r_squared
            }
            
            logger.info(f"\nüìà Scalability Analysis:")
            logger.info(f"   Linear coefficient: {a:.6f} seconds/row")
            logger.info(f"   R¬≤: {r_squared:.4f}")
            logger.info(f"   Is linear: {'‚úì Yes' if r_squared > 0.95 else '‚úó No'}")
            
            return analysis
        
        return {}


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ ETL –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    logger.info("="*60)
    logger.info("üöÄ Starting Big Data ETL Pipeline")
    logger.info("="*60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    pipeline = BigDataETLPipeline()
    
    try:
        # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        test_data_path = "./data/lake/raw/sample_data.csv"
        
        if os.path.exists(test_data_path):
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ
            df_raw = pipeline.extract_raw_data(test_data_path)
            
            # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
            df_transformed = pipeline.transform_large_dataset(df_raw)
            
            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è
            df_aggregated = pipeline.aggregate_by_dimensions(df_transformed)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Data Lake
            pipeline.load_to_data_lake(df_aggregated, layer='processed')
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Hive
            pipeline.load_to_hive(df_aggregated, 'goszakupki_aggregated')
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = pipeline.get_statistics()
        
    except Exception as e:
        logger.error(f"‚úó Error during ETL execution: {e}", exc_info=True)
    finally:
        pipeline.stop()
    
    logger.info("="*60)
    logger.info("‚úì ETL Pipeline completed")
    logger.info("="*60)


if __name__ == "__main__":
    main()
