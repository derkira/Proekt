{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# ГосЗакупки - Big Data ETL Pipeline\n",
    "\n",
    "## Полный сценарий работы системы\n",
    "\n",
    "Этот ноутбук демонстрирует работу полного Big Data конвейера с использованием:\n",
    "- Apache Hadoop (HDFS)\n",
    "- Apache Spark (обработка данных)\n",
    "- PostgreSQL (хранение)\n",
    "- Redis (кеширование)\n",
    "\n",
    "### План выполнения:\n",
    "1. Инициализация Spark сессии\n",
    "2. Генерация тестовых данных закупок\n",
    "3. Загрузка в HDFS\n",
    "4. Обработка и анализ\n",
    "5. Сохранение результатов\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Импорт необходимых библиотек\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, sum as spark_sum, avg, max as spark_max, min as spark_min\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"[OK] Все библиотеки импортированы успешно\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Инициализация Spark сессии\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"goszakupki-etl\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"[OK] Spark сессия инициализирована\")\n",
    "print(f\"    Master: spark://spark-master:7077\")\n",
    "print(f\"    App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"    Version: {spark.version}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Функция для генерации тестовых данных закупок\n",
    "\n",
    "def generate_zakupki_data(num_records: int) -> object:\n",
    "    \"\"\"Генерирует тестовые данные закупок\"\"\"\n",
    "    \n",
    "    categories = [\n",
    "        \"Медицина\", \"Транспорт\", \"Строительство\", \n",
    "        \"Энергетика\", \"IT\", \"Образование\", \n",
    "        \"Жилье\", \"Благоустройство\"\n",
    "    ]\n",
    "    \n",
    "    regions = [\n",
    "        \"Москва\", \"Санкт-Петербург\", \"Новосибирск\",\n",
    "        \"Екатеринбург\", \"Казань\", \"Челябинск\",\n",
    "        \"Омск\", \"Ростов-на-Дону\", \"Уфа\", \"Красноярск\"\n",
    "    ]\n",
    "    \n",
    "    statuses = [\"Планирование\", \"Объявлено\", \"Закрыто\", \"Отменено\"]\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'id': np.arange(1, num_records + 1),\n",
    "        'category': np.random.choice(categories, num_records),\n",
    "        'region': np.random.choice(regions, num_records),\n",
    "        'budget': np.random.uniform(100000, 15000000, num_records),\n",
    "        'date': [datetime.now() - timedelta(days=np.random.randint(0, 365)) \n",
    "                 for _ in range(num_records)],\n",
    "        'organization': [f\"Организация_{i}\" for i in np.random.randint(1, 1000, num_records)],\n",
    "        'status': np.random.choice(statuses, num_records),\n",
    "        'description': [f\"Закупка #{i}\" for i in range(1, num_records + 1)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return spark.createDataFrame(df)\n",
    "\n",
    "# Генерируем данные для демонстрации\n",
    "print(\"[*] Генерация тестовых данных...\")\n",
    "df_zakupki = generate_zakupki_data(10000)\n",
    "print(f\"[OK] Сгенерировано {df_zakupki.count()} записей\")\n",
    "print(f\"[OK] Колонки: {', '.join(df_zakupki.columns)}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Сохранение в HDFS\n",
    "\n",
    "hdfs_path = \"hdfs://namenode:9000/data/raw/zakupki\"\n",
    "\n",
    "print(f\"[*] Сохранение данных в HDFS ({hdfs_path})...\")\n",
    "\n",
    "try:\n",
    "    # Сохраняем в Parquet формат (оптимально для Spark)\n",
    "    df_zakupki.write.mode(\"overwrite\").parquet(hdfs_path)\n",
    "    print(f\"[OK] Данные сохранены в HDFS (Parquet формат)\")\n",
    "except Exception as e:\n",
    "    print(f\"[!] Ошибка при сохранении: {e}\")\n",
    "    print(f\"[INFO] Используем локальное сохранение для демонстрации\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Анализ данных по категориям\n",
    "\n",
    "print(\"[*] Анализ закупок по категориям...\")\n",
    "\n",
    "category_stats = df_zakupki.groupby('category').agg(\n",
    "    count('id').alias('количество'),\n",
    "    spark_sum('budget').alias('сумма'),\n",
    "    avg('budget').alias('средний_бюджет'),\n",
    "    spark_max('budget').alias('максимум'),\n",
    "    spark_min('budget').alias('минимум')\n",
    ").orderBy(col('сумма').desc())\n",
    "\n",
    "print(\"[OK] Результаты анализа по категориям:\")\n",
    "category_stats.show(truncate=False)\n",
    "\n",
    "# Преобразуем в Pandas для дальнейшего анализа\n",
    "category_pd = category_stats.toPandas()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Анализ по регионам\n",
    "\n",
    "print(\"[*] Анализ закупок по регионам...\")\n",
    "\n",
    "region_stats = df_zakupki.groupby('region').agg(\n",
    "    count('id').alias('количество'),\n",
    "    spark_sum('budget').alias('сумма'),\n",
    "    avg('budget').alias('средний_бюджет')\n",
    ").orderBy(col('сумма').desc())\n",
    "\n",
    "print(\"[OK] Результаты анализа по регионам:\")\n",
    "region_stats.show(truncate=False)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Фильтрация и трансформация\n",
    "\n",
    "print(\"[*] Фильтрация крупных закупок (бюджет > 5,000,000)...\")\n",
    "\n",
    "large_zakupki = df_zakupki.filter(col('budget') > 5000000) \\\n",
    "    .select('id', 'category', 'region', 'budget', 'organization', 'status') \\\n",
    "    .orderBy(col('budget').desc())\n",
    "\n",
    "print(f\"[OK] Найдено крупных закупок: {large_zakupki.count()}\")\n",
    "large_zakupki.show(10, truncate=False)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Статистика\n",
    "\n",
    "print(\"[*] Общая статистика по бюджету...\")\n",
    "\n",
    "stats = df_zakupki.agg(\n",
    "    spark_sum('budget').alias('всего'),\n",
    "    avg('budget').alias('среднее'),\n",
    "    spark_max('budget').alias('максимум'),\n",
    "    spark_min('budget').alias('минимум'),\n",
    "    count('id').alias('количество')\n",
    ")\n",
    "\n",
    "print(\"[OK] Статистика:\")\n",
    "stats.show(truncate=False)\n",
    "\n",
    "stats_pd = stats.toPandas()\n",
    "print(f\"\\nВсего бюджета: {stats_pd.iloc[0]['всего']:,.2f} RUB\")\n",
    "print(f\"Средний бюджет: {stats_pd.iloc[0]['среднее']:,.2f} RUB\")\n",
    "print(f\"Максимальный бюджет: {stats_pd.iloc[0]['максимум']:,.2f} RUB\")\n",
    "print(f\"Минимальный бюджет: {stats_pd.iloc[0]['минимум']:,.2f} RUB\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Сохранение результатов\n",
    "\n",
    "print(\"[*] Сохранение результатов анализа...\")\n",
    "\n",
    "# Сохраняем статистику по категориям\n",
    "category_pd.to_csv('/tmp/zakupki_category_stats.csv', index=False)\n",
    "print(\"[OK] Статистика по категориям сохранена\")\n",
    "\n",
    "# Сохраняем статистику по регионам\n",
    "region_pd = region_stats.toPandas()\n",
    "region_pd.to_csv('/tmp/zakupki_region_stats.csv', index=False)\n",
    "print(\"[OK] Статистика по регионам сохранена\")\n",
    "\n",
    "print(\"\\n[OK] ВСЕ РЕЗУЛЬТАТЫ ГОТОВЫ!\")\n",
    "print(\"Файлы:\")\n",
    "print(\"  - /tmp/zakupki_category_stats.csv\")\n",
    "print(\"  - /tmp/zakupki_region_stats.csv\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Завершение\n",
    "\n",
    "print(\"[*] Завершение Spark сессии...\")\n",
    "spark.stop()\n",
    "\n",
    "print(\"[OK] ======================================\")\n",
    "print(\"[OK] Конвейер ETL успешно выполнен!\")\n",
    "print(\"[OK] ======================================\")\n",
    "print(\"\\nСистема готова к использованию в production\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
